{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prace Domowe - Modelowanie Wieloagentowe "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podane są polecenia, w ramach obowiązkowej pracy domowej należy wybrać i wykonać cztery z nich, pozostałe można traktować jako dodatkowe zadania. Każde zadanie punktowane jest za 15 punktów. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 1\n",
    "\n",
    "Porównaj rozważane na zajęciach poświęconym  A/B testingowi trzy procedury próbkowania zmieniając horyzont czasowy oraz liczbę opcji.\n",
    "\n",
    "\n",
    "## Zadanie 2\n",
    "\n",
    "Korzystając ze środowiska <tt>Frozen Lake</tt> zaproponuj eksperyment, który pokaże złożoność obliczeniową prezentowanego na zajęciach rozwiązania problemu za pomocą programowania dynamicznego. Czy value iteration i policy iteration mają taka samą złożoność? Jakiego rzędu jest złożoność tych algorytmów?  \n",
    "\n",
    "\n",
    "## Zadanie 3\n",
    "\n",
    "W zaprezentowanym na ćwiczeniach środowisku <tt>Frozen Lake</tt> zaimplementuj algorytm Monte Carlo Off Policy Learning.\n",
    "\n",
    "## Zadanie 4\n",
    "\n",
    "Zmodyfikuj prezentowane na zajęciach kody do algorytmu Monte Carlo On Policy tak aby:\n",
    "\n",
    "   - parametr eksploracji $\\epsilon$ malał liniowo z czasem.\n",
    "   - parametr eksploracji $\\epsilon$ malał wykładniczo z czasem.\n",
    "   - zamiast parametru eksploracji $\\epsilon$ agent wykorzystywał górną granicę ufności.\n",
    "    \n",
    "które z proponowanych rozwiązań jest najefektywniejsze? Do oceny wykorzystaj dwie miary: ilość epizodów zakończonych sukcesem (dotarciem do mety) i przeciętna skumulowana nagroda agenta. Porównanie przeprowadź w horyzoncie 1000000 iteracji.\n",
    "          \n",
    "\n",
    "## Zadanie 5\n",
    "\n",
    "W prezentowanym na zajęciach przykładzie działania algorytmu $DynaQ$:\n",
    "\n",
    "1. Zmodyfikuj estymator modelu tak, aby działał efektywniej dla stochastycznego modelu.\n",
    "2. Zaimplementuj algorytm $DynaQ+$. Porównaj oba algorytmy (zmodyfikowany estymator i $DynaQ+$ dla stochastycznej wersji środowiska <tt>Frozen Lake</tt>.\n",
    "\n",
    "\n",
    "\n",
    "## Zadanie 6\n",
    "\n",
    "Zaprezentowana na zajęciach implementacja algorytmu aktora-krytyka w środowsku <tt>Cart Pole</tt> osiąga przeciętny wynik równy ok. 100 punktów. Problem uznaje się za rozwiązany gdy agent otrzymuje wynik powyżej 195 punktów w 100 kolejnych iteracjach. Spróbuj rozwiązać zadanie korzystając z dowolnego algorytmu.\n",
    "\n",
    "<b>Uwaga: Najlpsze rozwiązania zostaną nagrodzone dodatkowymi punktami. </b>\n",
    "\n",
    "\n",
    "## Zadanie 7\n",
    "\n",
    "Zaimplementuj algorytm aktora-krytyka w ciągłej wersji środowiska <tt>Mountain Car</tt>."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.2",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
