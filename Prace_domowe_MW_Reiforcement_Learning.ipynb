{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prace Domowe - Modelowanie Wieloagentowe "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podane są polecenia, w ramach obowiązkowej pracy domowej należy wybrać i wykonać cztery z nich, pozostałe można traktować jako dodatkowe zadania. Każde zadanie punktowane jest za 15 punktów. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 1\n",
    "\n",
    "Porównaj rozważane na zajęciach poświęconym  A/B testingowi trzy procedury próbkowania zmieniając horyzont czasowy oraz liczbę opcji.\n",
    "\n",
    "\n",
    "## Zadanie 2\n",
    "\n",
    "Korzystając ze środowiska <tt>Frozen Lake</tt> zaproponuj eksperyment, który pokaże złożoność obliczeniową prezentowanego na zajęciach rozwiązania problemu za pomocą programowania dynamicznego. Czy value iteration i policy iteration mają taka samą złożoność? Jakiego rzędu jest złożoność tych algorytmów?  \n",
    "\n",
    "\n",
    "## Zadanie 3\n",
    "\n",
    "W zaprezentowanym na ćwiczeniach środowisku <tt>Frozen Lake</tt> lub <tt>Blackjack</tt> zaimplementuj algorytm Monte Carlo Off Policy Learning.\n",
    "\n",
    "\n",
    "## Zadanie 4\n",
    "\n",
    "Zmodyfikuj kod od przykładu <tt>Blackjack</tt> on policy tak żeby kod uaktualniał wartość funkcji $Q(s,a)$ za każdą wizytą (<i>every visit</i>) zamiast uaktualniania przy każdej wizycie (<i>first visit</i>) tak jak jest teraz. W jaki sposób zmienia to oszacowania funkcji wartości i strategię? Odpowiedz na pytanie rozpatrując różne horyzonty czasowe.\n",
    "\n",
    "\n",
    "## Zadanie 5\n",
    "\n",
    "Prezentowany kod do gry w <tt>Blackjack</tt> zakłada, że krupier gra z predefiniowaną strategią (dobieraj karty dopóki suma twoich kart nie będzie większa bądź równa 17). Zmodyfikuj jego zachowanie tak aby on też uczył się optymalnej strategii (de facto zaimplementuj dwóch agentów grających ze sobą). W jaki sposób zmienia to strategie agenta(ów) i oszacowania funkcji wartości?\n",
    "\n",
    "<b>Uwaga: Rozwiązanie może wymagać modyfikacji środowiska, [tutaj](https://github.com/JuliaReinforcementLearning/ReinforcementLearningAnIntroduction.jl/blob/master/notebooks/Chapter05_Blackjack.jl) dostępny jest przykład jego implementacji w języku Julia, który możesz wykorzystać </b>\n",
    "\n",
    "\n",
    "## Zadanie 6\n",
    "\n",
    "Zmodyfikuj prezentowane na zajęciach kody do algorytmu Monte Carlo On Policy w środowisku <tt>Frozen Lake</tt> tak aby:\n",
    "\n",
    "   - parametr eksploracji $\\epsilon$ malał liniowo z czasem.\n",
    "   - parametr eksploracji $\\epsilon$ malał wykładniczo z czasem.\n",
    "   - zamiast parametru eksploracji $\\epsilon$ agent wykorzystywał górną granicę ufności.\n",
    "    \n",
    "które z proponowanych rozwiązań jest najefektywniejsze? Do oceny wykorzystaj dwie miary: ilość epizodów zakończonych sukcesem (dotarciem do mety) i przeciętna skumulowana nagroda agenta. Porównanie przeprowadź w horyzoncie 1000000 iteracji.\n",
    "\n",
    "## Zadanie 7\n",
    "\n",
    "Na zajęciach pokazaliśmy, że dla stałej wartości parametru eksploracji $\\epsilon$ algorytm $SARSA$ uczy się rozwiązania suboptymalnego. Zmodyfikuj parametr $\\epsilon$ tak aby malał z czasem. Czy w takim wypadku $SARSA$ uczy się rozwiązania optymalnego? Jak wyglądają przeciętne nagrody dla obu algorytmów ($SARSA$ i $Q - learning$) gdy $\\epsilon$ maleje? Porównaj je na wykresie z wynikami z zajęć.\n",
    "\n",
    "\n",
    "## Zadanie 8\n",
    "\n",
    "W środowisku <tt>GridWorld</tt>  z zajęć zaimplementuj algorytm $SARSA(\\lambda)$ ze śladami wybieralności (<i>eligibility  traces</i>). Porównaj jego efektywność z  bazowym algorytmem $SARSA(0)$.\n",
    "\n",
    "\n",
    "## Zadanie 9\n",
    "\n",
    "W prezentowanym na zajęciach przykładzie działania algorytmu $DynaQ$, zaimplementuj algorytm $DynaQ+$. Porównaj oba algorytmy. Czy wynik dla dłuższych okresów planowania poprawia się?\n",
    "\n",
    "\n",
    "## Zadanie 10\n",
    "\n",
    "Zmodyfikuj prezentowany na zajęciach kod z liniową aproksymacją funkcji wartości w środowisku <tt>Mountain Car</tt> tak aby uczył się korzystając z algorytmu $Q-learning$, a nie algorytmu $SARSA$. Czy algorytm zbiega do poprawnego rozwiązania?\n",
    "\n",
    "## Zadanie 11\n",
    "\n",
    "Zmodyfikuj prezentowany na zajęciach kod z liniową aproksymacją funkcji wartości w środowisku <tt>Mountain Car</tt> tak aby kodował przestrzeń stanów wykorzystując do tego radialne funkcje bazowe. Jak zmienia się rozwiązanie? \n",
    "\n",
    "## Zadanie 12\n",
    "\n",
    "Zmodyfikuj prezentowany na zajęciach kod z liniową aproksymacją funkcji wartości w środowisku <tt>Mountain Car</tt> tak aby uczył się za pomocą algorytmu $REINFORCE$. Jak zachowuje się rozwiązanie?\n",
    "\n",
    "## Zadanie 13\n",
    "\n",
    "Zaprezentowana na zajęciach implementacja algorytmu aktora-krytyka w środowsku <tt>Cart Pole</tt> osiąga przeciętny wynik równy ok. 100 punktów. Problem uznaje się za rozwiązany gdy agent otrzymuje wynik powyżej 195 punktów w 100 kolejnych iteracjach. Spróbuj rozwiązać zadanie korzystając z dowolnego algorytmu.\n",
    "\n",
    "<b>Uwaga: Najlepsze rozwiązania zostaną nagrodzone dodatkowymi punktami. </b>\n",
    "\n",
    "\n",
    "## Zadanie 14\n",
    "\n",
    "Zaimplementuj algorytm aktora-krytyka w ciągłej wersji środowiska <tt>Mountain Car</tt>."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.2",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
