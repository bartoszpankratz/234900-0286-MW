{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prace Domowe - Modelowanie Wieloagentowe "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podane są polecenia, w ramach obowiązkowej pracy domowej należy wybrać i wykonać sześć z nich, pozostałe można traktować jako dodatkowe zadania. Każde zadanie punktowane jest za 15 punktów. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 1\n",
    "\n",
    "Korzystając ze środowiska <tt>Frozen Lake</tt> zaproponuj eksperyment, który pokaże złożoność obliczeniową prezentowanego na zajęciach rozwiązania problemu za pomocą programowania dynamicznego. Czy value iteration i policy iteration mają taka samą złożoność? Jakiego rzędu jest złożoność tych algorytmów?  \n",
    "\n",
    "\n",
    "## Zadanie 2\n",
    "\n",
    "W zaprezentowanym na ćwiczeniach środowisku <tt>Frozen Lake</tt> lub <tt>Blackjack</tt> zaimplementuj algorytm Monte Carlo Off Policy Learning.\n",
    "\n",
    "\n",
    "## Zadanie 3\n",
    "\n",
    "Zmodyfikuj kod od przykładu <tt>Blackjack</tt> on policy tak żeby kod uaktualniał wartość funkcji $Q(s,a)$ za każdą wizytą (<i>every visit</i>) zamiast uaktualniania przy każdej wizycie (<i>first visit</i>) tak jak jest teraz. W jaki sposób zmienia to oszacowania funkcji wartości i strategię? Odpowiedz na pytanie rozpatrując różne horyzonty czasowe.\n",
    "\n",
    "## Zadanie 4\n",
    "\n",
    "Dla najlepszego rozwiązania z zadania 2 (lub dowolnie wybranej metody spadku parametru eksploracji z poprzedniego zadania): \n",
    "- przyjmij, że agent nauczył się odpowiedniej strategii gdy jego success rate na przestrzeni 100 ostatnich iteracji przekracza wartośc $0.8\\times(1-\\epsilon)$. Po osiągnięciu takiego wyniku niech agent przestanie się uczyć i zacznie eksploatować swoją wyuczoną strategię. Zmodyfikuj kod aby uwzględnić to założenie. \n",
    "- przeprowadź eksperyment, który porówna ile epizodów jest potrzebnych do nauczenia się strategii w powyższym przypadku z ilością iteracji potrzebnych do rozwiązania zadania korzystając z algorytmu policy iteration. Przyjmij następujące parametry: wielkość jezior $dim \\in [100,200,300,\\dots,10\\_000]$ i $p\\_holes = 0.2$\n",
    "\n",
    "## Zadanie 5\n",
    "\n",
    "Prezentowany kod do gry w <tt>Blackjack</tt> zakłada, że krupier gra z predefiniowaną strategią (dobieraj karty dopóki suma twoich kart nie będzie większa bądź równa 17). Zmodyfikuj jego zachowanie tak aby on też uczył się optymalnej strategii (de facto zaimplementuj dwóch agentów grających ze sobą). W jaki sposób zmienia to strategie agenta(ów) i oszacowania funkcji wartości?\n",
    "\n",
    "<b>Uwaga: Rozwiązanie może wymagać modyfikacji środowiska, [tutaj](https://github.com/JuliaReinforcementLearning/ReinforcementLearningAnIntroduction.jl/blob/master/notebooks/Chapter05_Blackjack.jl) dostępny jest przykład jego implementacji w języku Julia, który możesz wykorzystać </b>\n",
    "\n",
    "\n",
    "## Zadanie 6\n",
    "\n",
    "Zmodyfikuj prezentowane na zajęciach kody do algorytmu Monte Carlo On Policy w środowisku <tt>Frozen Lake</tt> tak aby:\n",
    "\n",
    "   - parametr eksploracji $\\epsilon$ malał liniowo z czasem.\n",
    "   - parametr eksploracji $\\epsilon$ malał wykładniczo z czasem.\n",
    "   - zamiast parametru eksploracji $\\epsilon$ agent wykorzystywał górną granicę ufności.\n",
    "    \n",
    "które z proponowanych rozwiązań jest najefektywniejsze? Do oceny wykorzystaj dwie miary: ilość epizodów zakończonych sukcesem (dotarciem do mety) i przeciętna skumulowana nagroda agenta. Porównanie przeprowadź w horyzoncie 1000000 iteracji.\n",
    "\n",
    "## Zadanie 7\n",
    "\n",
    "Na zajęciach pokazaliśmy, że dla stałej wartości parametru eksploracji $\\epsilon$ algorytm $SARSA$ uczy się rozwiązania suboptymalnego. Zmodyfikuj parametr $\\epsilon$ tak aby malał z czasem. Czy w takim wypadku $SARSA$ uczy się rozwiązania optymalnego? Jak wyglądają przeciętne nagrody dla obu algorytmów ($SARSA$ i $Q - learning$) gdy $\\epsilon$ maleje? Porównaj je na wykresie z wynikami z zajęć.\n",
    "\n",
    "\n",
    "## Zadanie 8\n",
    "\n",
    "W środowisku <tt>GridWorld</tt>  z zajęć zaimplementuj algorytm $SARSA(\\lambda)$ ze śladami wybieralności (<i>eligibility  traces</i>). Porównaj jego efektywność z  bazowym algorytmem $SARSA(0)$ dla różnych wartości parametru $\\lambda$.\n",
    "\n",
    "\n",
    "## Zadanie 9\n",
    "\n",
    "W prezentowanym na zajęciach przykładzie działania algorytmu $DynaQ$, zaimplementuj algorytm $DynaQ+$. Porównaj oba algorytmy. Czy wynik dla dłuższych okresów planowania poprawia się?\n",
    "\n",
    "\n",
    "## Zadanie 10\n",
    "\n",
    "Zmodyfikuj prezentowany na zajęciach kod z liniową aproksymacją funkcji wartości w środowisku <tt>Mountain Car</tt> tak aby uczył się korzystając z algorytmu $Q-learning$, a nie algorytmu $SARSA$. Czy algorytm zbiega do poprawnego rozwiązania?\n",
    "\n",
    "## Zadanie 10\n",
    "\n",
    "Zmodyfikuj prezentowany na zajęciach kod z liniową aproksymacją funkcji wartości w środowisku <tt>Mountain Car</tt> tak aby kodował przestrzeń stanów wykorzystując do tego radialne funkcje bazowe. Jak zmienia się rozwiązanie? \n",
    "\n",
    "## Zadanie 11\n",
    "\n",
    "Zmodyfikuj prezentowany na zajęciach kod z liniową aproksymacją funkcji wartości w środowisku <tt>Mountain Car</tt> tak aby uczył się za pomocą algorytmu $REINFORCE$. Jak zachowuje się rozwiązanie?\n",
    "\n",
    "\n",
    "## Zadanie 12\n",
    "\n",
    "Zaimplementuj algorytm aktora-krytyka w ciągłej wersji środowiska <tt>Mountain Car</tt>."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.0",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
