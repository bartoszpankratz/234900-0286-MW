{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "495c1fb8",
   "metadata": {},
   "source": [
    "# Continous actions Actor Critic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd17b3a",
   "metadata": {},
   "source": [
    "Based on example avalaible [here](https://cs.stanford.edu/people/karpathy/convnetjs/demo/rldemo.html). Codes for original case are avalaible [here](https://github.com/karpathy/convnetjs/blob/4c3358a315b4d71f31a0d532eb5d1700e9e592ee/demo/js/rldemo.js)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8b3fc6",
   "metadata": {},
   "source": [
    "Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccbdb5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots, LinearAlgebra, ReinforcementLearning, IntervalSets\n",
    "using Flux\n",
    "using Flux: params\n",
    "using Plots; \n",
    "gr()\n",
    "import StatsBase.sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f29940",
   "metadata": {},
   "source": [
    "Firstly, let us define objects existing in the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75fafea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ball"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Balls object - red ones (type \"0\") are poisonous and green ones (type \"1\") are edible \n",
    "mutable struct Ball{TI<:Integer, TF<:AbstractFloat}\n",
    "    kind::TI #1 poison, 2 food\n",
    "    loc::NamedTuple{(:x, :y), Tuple{TF, TF}}\n",
    "    radius::Float64\n",
    "    age::TI\n",
    "end\n",
    "\n",
    "\n",
    "#balls constructor\n",
    "Ball(k) = k ∈ [1,2] ? Ball(k,(x =rand(), y = rand()),0.01, rand(1:100)) : @error \"wrong type of object - it could be 1 or 2!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899907de",
   "metadata": {},
   "source": [
    "And sensors that agent use to interact with the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a344c6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#definition of agent's eye:\n",
    "mutable struct Eye{TI<:Integer, TF<:AbstractFloat}\n",
    "    angle::TF\n",
    "    max_range::TF\n",
    "    sensed_proximity::TF\n",
    "    sensed_type::TI #0 nothing; 1 poison, 2 food, 3 wall\n",
    "end\n",
    "\n",
    "Eye(a) = Eye(a, 0.2,0.2, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d40020",
   "metadata": {},
   "source": [
    "Now, we could proceed with defining an environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ac8a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct ContinuousLabirynthEnv <: AbstractEnv\n",
    "    walls::Vector{Vector{NamedTuple{(:x, :y), Tuple{Float64, Float64}}}}\n",
    "    nb::Int64\n",
    "    balls::Vector{Ball{Int64, Float64}}\n",
    "    observation_space::Space{Vector{ClosedInterval{Float64}}}\n",
    "    action_space::Space{Vector{ClosedInterval{Float64}}}\n",
    "    velocity::Vector{Float64}\n",
    "    old_position::NamedTuple{(:x, :y), Tuple{Float64, Float64}}\n",
    "    position::NamedTuple{(:x, :y), Tuple{Float64, Float64}}\n",
    "    radius::Float64\n",
    "    angle::Float64\n",
    "    eyes::Vector{Eye{Int64, Float64}}\n",
    "    digestion_signal::Float64\n",
    "end\n",
    "Main.ContinuousLabirynthEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2568fc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "function ContinuousLabirynthEnv(nb; radius = 0.01)\n",
    "    walls = [#bounds of the map:\n",
    "        [(x = 0.0, y = 0.0),(x = 0.0, y = 1.0)],\n",
    "        [(x = 0.0, y = 0.0),(x = 1.0, y = 0.0)],\n",
    "        [(x = 1.0, y = 0.0),(x = 1.0, y = 1.0)],\n",
    "        [(x = 0.0, y = 1.0),(x = 1.0, y = 1.0)],\n",
    "        #walls inside the map:\n",
    "        [(x = 0.1, y = 0.1),(x = 0.3, y = 0.1)],\n",
    "        [(x = 0.1, y = 0.9),(x = 0.3, y = 0.9)],\n",
    "        [(x = 0.3, y = 0.1),(x = 0.3, y = 0.9)],\n",
    "        \n",
    "        [(x = 0.7, y = 0.1),(x = 0.9, y = 0.1)],\n",
    "        [(x = 0.7, y = 0.9),(x = 0.9, y = 0.9)],\n",
    "        #[(x = 0.9, y = 0.1),(x = 0.9, y = 0.9)], #can be use instead of the latter one:\n",
    "        [(x = 0.7, y = 0.1),(x = 0.7, y = 0.9)]\n",
    "        ]\n",
    "    balls = [Ball(rand([1,2])) for i = 1:nb]\n",
    "    position = (x = rand(), y = rand());\n",
    "    eyes = [Eye((k - 4)*0.25) for k = 1:9];\n",
    "    observation_space = Space([0.0..1.0 for i = 1:(3 * length(eyes))])\n",
    "    action_space = Space([0.0..1.0, 0.0..1.0])\n",
    "    return ContinuousLabirynthEnv(walls, nb, balls, observation_space, action_space, [0.0, 0.0],\n",
    "            position, position, radius, 0.0, eyes, 0.0)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad0d97b",
   "metadata": {},
   "source": [
    "Auxilliary functions - physics of the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa8033a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rotate a vector clockwise https://en.wikipedia.org/wiki/Rotation_matrix\n",
    "\n",
    "rot(vec, angle) = [cos(angle) sin(angle); -sin(angle) cos(angle)] * vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8dd6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#line circle intersect: \n",
    "#https://codereview.stackexchange.com/questions/86421/line-segment-to-circle-collision-algorithm\n",
    "function intersect(vec,centre,radius)\n",
    "    v = values(vec[2]) .- values(vec[1])\n",
    "    a = sum(v.*v)\n",
    "    b = 2* sum(v .* (values(vec[1]) .- values(centre)))\n",
    "    c =  sum(values(vec[1]) .* values(vec[1])) + sum(values(centre) \n",
    "        .* values(centre)) - 2 * sum(values(vec[1]) .* values(centre)) - radius ^2\n",
    "    Δ = b^2 - 4*a*c\n",
    "    if Δ < 0\n",
    "        return false\n",
    "    else\n",
    "        t₁ = (-b + √Δ)/(2*a)\n",
    "        t₂ = (-b - √Δ)/(2*a)\n",
    "        if 0.0 ≤ t₁ ≤ 1 || 0.0 ≤ t₂ ≤ 1\n",
    "            p₁= values(vec[1]) .+ t₁ .* v\n",
    "            p₂ = values(vec[1]) .+ t₂ .* v\n",
    "            norm(p₁ .- values(vec[1])) < norm(p₂ .- values(vec[1])) ? (return p₁) : (return p₂)\n",
    "        else\n",
    "            return false\n",
    "        end \n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f84c38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#interscetion of two lines:\n",
    "#http://www-cs.ccny.cuny.edu/~wolberg/capstone/intersection/Intersection%20point%20of%20two%20lines.html\n",
    "function intersect(vec1,vec2)\n",
    "    denominator = (vec2[2].y - vec2[1].y)*(vec1[2].x - vec1[1].x) - (vec2[2].x - vec2[1].x)*(vec1[2].y - vec1[1].y)\n",
    "    denominator == 0.0 && (return false)\n",
    "    u₁ = ((vec2[2].x - vec2[1].x)*(vec1[1].y - vec2[1].y) - (vec2[2].y - vec2[1].y)*(vec1[1].x - vec2[1].x)) / denominator\n",
    "    u₂ = ((vec1[2].x - vec1[1].x)*(vec1[1].y - vec2[1].y) - (vec1[2].y - vec1[1].y)*(vec1[1].x - vec2[1].x)) / denominator\n",
    "    (0.0 ≤ u₁ ≤ 1.0 &&  0.0 ≤ u₂ ≤ 1.0) ? (return (vec1[1].x + u₁ * (vec1[2].x - vec1[1].x),\n",
    "                                                    vec1[1].y + u₁ * (vec1[2].y - vec1[1].y))) : (return false) \n",
    "end\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489b2007",
   "metadata": {},
   "source": [
    "Other auxilliary functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9f99fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vector of agent's eyesight:\n",
    "eyesight(env, eye) = [env.position, (x = env.position.x + sin(env.angle + eye.angle)*eye.sensed_proximity, \n",
    "                            y = env.position.y + cos(env.angle + eye.angle)*eye.sensed_proximity)]\n",
    "\n",
    "eyesight(env) = [eyesight(env,eye) for eye in env.eyes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961cd37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting \n",
    "function plot(env::ContinuousLabirynthEnv)\n",
    "    p = Plots.plot(framestyle = :none, legend=:none)\n",
    "    \n",
    "    for wall in env.walls\n",
    "        plot!([w.x for w in wall],[w.y for w in wall], linewidth=2, c = :blue)\n",
    "    end\n",
    "    \n",
    "    scatter!([ball.loc.x for ball in env.balls if ball.kind == 1], \n",
    "            [ball.loc.y for ball in env.balls if ball.kind == 1], \n",
    "            color=:red, m = :circle, markersize=16, alpha=0.6)\n",
    "\n",
    "    scatter!([ball.loc.x for ball in env.balls if ball.kind == 2], \n",
    "            [ball.loc.y for ball in env.balls if ball.kind == 2], \n",
    "            color=:green, m = :circle, markersize=16, alpha=0.6)\n",
    "    \n",
    "    scatter!([env.position.x,], [env.position.y,], color=:orange, markersize=20, alpha=0.8)\n",
    "\n",
    "    for eye in eyesight(env) \n",
    "        plot!([e.x for e in eye],[e.y for e in eye], linewidth=0.5, c = :black,alpha=0.6)\n",
    "    end\n",
    "    display(p)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcac814c",
   "metadata": {},
   "source": [
    "Finally, the most important functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aca158d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions:\n",
    "RLBase.action_space(env::ContinuousLabirynthEnv) = env.action_space\n",
    "\n",
    "\"\"\"\n",
    "    state_space(env::ContinuousLabirynthEnv)\n",
    "\n",
    "State is represented as  vector three times number of eyes and is coded as follows:\n",
    "-every three neighboring values are representing a signal visible by one eye, e.g. \n",
    " [1.0, 1.0, 0.14] means that agent do not see any edible balls (first value of vector)\n",
    "nor poisons (second value of vector), but see the wall in the proximity of 0.14\n",
    "\"\"\"\n",
    "RLBase.state_space(env::ContinuousLabirynthEnv) = env.observation_space\n",
    "\n",
    "function RLBase.state(env::ContinuousLabirynthEnv) \n",
    "    state = ones(Float64, length(env.observation_space))\n",
    "    for (i, eye) in enumerate(env.eyes)\n",
    "        eye.sensed_type == 0 && continue \n",
    "        state[(i-1)*3 + eye.sensed_type] = eye.sensed_proximity/eye.max_range\n",
    "    end\n",
    "    return state\n",
    "end\n",
    "\n",
    "function RLBase.reward(env::ContinuousLabirynthEnv)\n",
    "    #agent do not like to see walls, especially up close:\n",
    "    proximity_reward = 0.0\n",
    "    for eye in env.eyes\n",
    "        if eye.sensed_type == 3\n",
    "            proximity_reward -= 1 - eye.sensed_proximity/eye.max_range\n",
    "        elseif eye.sensed_type != 0\n",
    "            proximity_reward += 1 - eye.sensed_proximity/eye.max_range\n",
    "        end\n",
    "        proximity_reward = proximity_reward / length(env.eyes)\n",
    "    end\n",
    "    #agent like to go straight forward:\n",
    "    forward_reward = 0.0\n",
    "    if env.old_position == env.position \n",
    "        forward_reward += -1.0\n",
    "    else\n",
    "        forward_reward += 1.0\n",
    "    end\n",
    "    ##agent also like to eat good things:\n",
    "    return env.digestion_signal + proximity_reward + forward_reward\n",
    "end\n",
    "\n",
    "RLBase.is_terminated(env::ContinuousLabirynthEnv) = false\n",
    "function RLBase.reset!(env::ContinuousLabirynthEnv) \n",
    "    env.position = (x = rand(), y = rand());\n",
    "    env.angle = 0.0\n",
    "    env.digestion_signal = 0.0\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f4aaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if agent do not fall from the map:\n",
    "on_boundary(vec) = any(vec .≤ 0.0) || any(vec .≥ 1.0)\n",
    "\n",
    "#check if agent do not collide with a wall:\n",
    "function collide(env, vec)\n",
    "    for wall in env.walls\n",
    "        if intersect(wall,vec,env.radius) != false \n",
    "            env.angle  += π/2 #we turn agent 180° to avoid him stucking in the wall \n",
    "            return true \n",
    "        end\n",
    "    end\n",
    "    return false\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1e0ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#movement function of agent\n",
    "function turnaround_and_move!(env, action)\n",
    "    #agent's movement is caused by the two  wheels rotating with different velocities and directions (action) \n",
    "    #wheels are perpendicular to the direction agent is facing at the moment\n",
    "    \n",
    "    #firstly we must find the positions of the wheels\n",
    "    #we will rotate agent's radius by 90°:\n",
    "    vec = rot([0.0, env.radius],env.angle + π/2)\n",
    "    #now, we could find the positions of both wheels:\n",
    "    wheel_1 = [env.position.x + vec[1], env.position.y + vec[2]] \n",
    "    wheel_2 = [env.position.x - vec[1], env.position.y - vec[2]] \n",
    "    #rotate first wheel, clockwise:\n",
    "    vel_clockwise = rot(-1 .* vec, action.v1)\n",
    "    #rotate second wheel, counterclockwise:\n",
    "    vel_counterclockwise = rot(vec, -action.v2)\n",
    "    #new position of first wheel:\n",
    "    wheel_1 = [wheel_1[1] + vel_clockwise[1], wheel_1[2] + vel_clockwise[2]]\n",
    "    #new position of second wheel:\n",
    "    wheel_2 = [wheel_2[1] + vel_counterclockwise[1], wheel_2[2] + vel_counterclockwise[2]]\n",
    "    #new position of agent is just an average of the positions of both wheels:\n",
    "    new_position = [wheel_1[1] + wheel_2[1], wheel_1[2] + wheel_2[2]] ./ 2\n",
    "    \n",
    "    #now we must adjust the angle that agent is facing:\n",
    "    env.angle += action.v1\n",
    "    env.angle > 2*π && (env.angle -= 2 * π)\n",
    "    env.angle -= action.v2\n",
    "    env.angle < 0 && (env.angle += 2 * π)\n",
    "    \n",
    "    #and check if new position is feasible:\n",
    "    if !(collide(env, new_position) || on_boundary(new_position))\n",
    "        env.old_position = deepcopy(env.position)\n",
    "        env.position  = (x = new_position[1], y = new_position[2])\n",
    "   end \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e0cb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#uaktualniamy obiekty, ktore widzi agent:\n",
    "#dla kazdego oka sprawdzamy czy widzi jakis objekt lub sciane, jezeli tak to w jakiej odleglosci\n",
    "#dodatkowo patrzymy czy agent nic nie zjadl - nie przecial sie z kulka oznaczajaca jedzenie\n",
    "function look_at_things_and_eat!(env)\n",
    "    for eye in env.eyes\n",
    "        eye.sensed_proximity = eye.max_range\n",
    "        eye.sensed_type = 0\n",
    "        eye_sight = eyesight(env,eye)\n",
    "        for wall in env.walls\n",
    "            intersect_point = intersect(eye_sight,wall)\n",
    "            intersect_point == false && continue \n",
    "            proximity = norm(values(env.position) .- intersect_point)\n",
    "            if proximity < eye.sensed_proximity\n",
    "                eye.sensed_proximity = proximity\n",
    "                eye.sensed_type = 3\n",
    "            end\n",
    "        end\n",
    "        for ball in env.balls\n",
    "            intersect_point = intersect(eye_sight,ball.loc, ball.radius)\n",
    "            intersect_point == false && continue \n",
    "            proximity = norm(values(env.position) .- intersect_point)\n",
    "            if proximity < eye.sensed_proximity\n",
    "                if proximity < (env.radius + ball.radius)\n",
    "                    ball.kind == 2 ? (env.digestion_signal += 5.0) : (env.digestion_signal -= 6.0)\n",
    "                    ball.age = 9999999999999 \n",
    "                else\n",
    "                    eye.sensed_proximity = proximity\n",
    "                    eye.sensed_type = ball.kind\n",
    "                end\n",
    "            end \n",
    "        end\n",
    "    end\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510e0805",
   "metadata": {},
   "outputs": [],
   "source": [
    "function update_balls!(env)\n",
    "    for ball in env.balls\n",
    "        ball.age += 1\n",
    "    end\n",
    "    env.balls = filter(ball -> ball.age < 500, env.balls)\n",
    "    for new_ball in 1:(env.nb - length(env.balls))\n",
    "        push!(env.balls, Ball(rand([1,2])))\n",
    "    end    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4e41a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "function (x::ContinuousLabirynthEnv)(action)\n",
    "    @assert action in action_space(x)\n",
    "    x.velocity = [action.v1, action.v2]\n",
    "    x.digestion_signal = 0.0\n",
    "    turnaround_and_move!(x, action)\n",
    "    look_at_things_and_eat!(x)\n",
    "    update_balls!(x)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0291dd99",
   "metadata": {},
   "source": [
    "Now, time for brain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108fc3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct Brain\n",
    "    experience_size::Int64\n",
    "    experience::Array\n",
    "    min_experience_size::Int64\n",
    "    net::Flux.Chain\n",
    "    μ::Dense\n",
    "    logσ::Dense\n",
    "    value_net::Flux.Chain\n",
    "    ηₚ::Float64\n",
    "    ηᵥ::Float64\n",
    "    β::Float64\n",
    "    batch_size::Int64\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd843a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "function Brain(input_size; experience_size = 3000, min_memory_size = 1000, ηₚ = .01, ηᵥ = 0.001, β = 0.95, batch_size = 64)\n",
    "    net = Chain(Dense(input_size,128,relu),Dense(128, 64,relu),\n",
    "        Dense(64, 32, identity))\n",
    "    μ = Dense(32, 2, sigmoid)\n",
    "    logσ = Dense(32, 2, sigmoid)\n",
    "    value_net = Chain(Dense(input_size,128,identity),\n",
    "        Dense(128, 64,relu),Dense(64, 32,relu),Dense(32, 1,identity));\n",
    "    return Brain(experience_size, [], min_memory_size, net, μ, logσ, value_net, ηₚ, ηᵥ, β, batch_size)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b72d99e",
   "metadata": {},
   "source": [
    "and Agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb25880c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct ContinuousAgent\n",
    "    env::ContinuousLabirynthEnv \n",
    "    brain::Brain\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0aa8190",
   "metadata": {},
   "outputs": [],
   "source": [
    "function ContinuousAgent(nb)\n",
    "    env = ContinuousLabirynthEnv(nb)\n",
    "    brain = Brain(length(state_space(env)))\n",
    "    return ContinuousAgent(env,brain)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c3f2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "function forward(brain, state)\n",
    "    v = brain.value_net(state)\n",
    "    Z = brain.net(state)\n",
    "    mu = brain.μ(Z)\n",
    "    logsigma = brain.logσ(Z)\n",
    "    return v[1], mu, logsigma\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603cc63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "function remember!(brain::Brain, step::Tuple)\n",
    "    length(brain.experience) == brain.experience_size && deleteat!(brain.experience,1)\n",
    "    push!(brain.experience, step)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a0bdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "critic_loss(x, y, ξ = 0.5) = ξ*Flux.mse(agent.brain.value_net(x), y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130302c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "function gaussian_loss(s, a, v, γ = 0.001)\n",
    "    Z = agent.brain.net(s)\n",
    "    mu = agent.brain.μ(Z)\n",
    "    sigma = exp.(agent.brain.logσ(Z))\n",
    "    pdf_val = 1 ./ (sigma .* sqrt(2*π)) .* exp.(-0.5 .* ((a .- mu) ./ sigma).^2)\n",
    "    log_prob = log.(pdf_val .+ 1e-7)\n",
    "    entropy = sum(-log.(pdf_val .+ 1e-7) .* pdf_val) / length(log_prob)\n",
    "    return sum(-log_prob .* v) / length(log_prob) - γ * entropy\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfd64bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "function replay!(agent::ContinuousAgent)\n",
    "    S = zeros(Float32,length(state(agent.env)), agent.brain.batch_size)\n",
    "    A = zeros(Float32,length(action_space(agent.env)), agent.brain.batch_size)\n",
    "    Adv = zeros(Float32, 1, agent.brain.batch_size)\n",
    "    V = zeros(Float32,1, agent.brain.batch_size)\n",
    "    for (i,step)  in enumerate(sample(agent.brain.experience, agent.brain.batch_size, replace = false))\n",
    "        s,a,r,s′,v,v′ = step\n",
    "        R = r + agent.brain.β * v′\n",
    "        adv = R - v\n",
    "        S[:, i] .= s\n",
    "        A[:, i] .= a\n",
    "        Adv[:, i] .= adv\n",
    "        V[:, i] .= R\n",
    "    end\n",
    "    \n",
    "    Flux.train!(gaussian_loss, params(agent.brain.net, agent.brain.μ, agent.brain.logσ), [(S,A,Adv)], ADAM(agent.brain.ηₚ))\n",
    "    Flux.train!(critic_loss, params(agent.brain.value_net), [(S, V)], ADAM(agent.brain.ηᵥ))\n",
    "    #push!(agent.losses, gaussian_loss(S,A,R))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447048d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "function step!(agent::ContinuousAgent,  training::Bool)\n",
    "    s = deepcopy(state(agent.env))\n",
    "    v, μ,logσ = forward(agent.brain, s)\n",
    "    a = μ + exp.(logσ) .* randn(length(logσ))\n",
    "    a = (Flux.tanh.(a) .+1 ) ./ 2\n",
    "    agent.env((v1 = a[1], v2 = a[2]))\n",
    "    r = deepcopy(reward(agent.env))\n",
    "    s′ = deepcopy(state(agent.env))\n",
    "    v′,_ ,_ = forward(agent.brain, s′)\n",
    "    remember!(agent.brain, (s,a,r,s′,v,v′))\n",
    "    (training && length(agent.brain.experience) > agent.brain.min_experience_size) && replay!(agent)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94188e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "function run!(agent::ContinuousAgent, steps::Int; training::Bool = true,\n",
    "            plotting::Bool = true, summary::Bool = true)\n",
    "    step = 1.0\n",
    "    while step ≤ steps\n",
    "        plotting && (plot(agent.env); sleep(0.0001))\n",
    "        step!(agent, training)\n",
    "        if summary && mod(step,100) == 0\n",
    "            @info \"step $(Int(step))\"\n",
    "            @info \"Reward: $(reward(agent.env))\"\n",
    "            s,a,r,s′,v,v′ = agent.brain.experience[end]\n",
    "            @info \"actor loss: $(gaussian_loss(s,a, (r + agent.brain.β *  v′) - v))\"\n",
    "            @info \"critc loss: $(critic_loss(s,r + agent.brain.β *  v′))\"\n",
    "        end\n",
    "        step += 1.0\n",
    "    end\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b1afcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ContinuousAgent(50);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03c1e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "v, μ,logσ = forward(agent.brain, state(agent.env))\n",
    "        a = μ + exp.(logσ) .* randn(length(logσ))\n",
    "        a = (Flux.tanh.(a) .+1 ) ./ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f896846f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "agent = ContinuousAgent(50)\n",
    "run!(agent,1_000_000, plotting = false, summary = true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98bc13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run!(agent,100, plotting = true,training = false, summary = true)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.5",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
