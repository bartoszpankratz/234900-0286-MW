{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af141019",
   "metadata": {},
   "source": [
    "# Frozen Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d70ae607",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modified frozen lake example, see: https://gym.openai.com/envs/FrozenLake-v0/\n",
    "\n",
    "using Random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954ae17e",
   "metadata": {},
   "source": [
    "### Auxilliary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "128ec54d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "print_policy (generic function with 2 methods)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# actions coded as :left => 1, :down => 2, :right => 3, :up => 4\n",
    "actions = Dict(1 => [0,-1], 2 => [1,0], 3 => [0,1], 4 => [-1,0]);\n",
    "\n",
    "#arrows are corresponding to actions\n",
    "arrows = Dict(1 => '⇐', 2 => '⇓', 3 => '⇒', 4 => '⇑');\n",
    "\n",
    "rewards = Dict('S' => -0.05, 'G' => 1.0, 'H' => -1.0, 'F' => -0.05);\n",
    "\n",
    "grid4x4= ['S' 'F' 'F' 'F';\n",
    "        'F' 'H' 'F' 'H';\n",
    "        'F' 'F' 'F' 'H';\n",
    "        'H' 'F' 'F' 'G'];\n",
    "\n",
    "grid8x8 =['S' 'F' 'F' 'F' 'F' 'F' 'F' 'F';\n",
    "        'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F';\n",
    "        'F' 'F' 'F' 'H' 'F' 'F' 'F' 'F';\n",
    "        'F' 'F' 'F' 'F' 'F' 'H' 'F' 'F';\n",
    "        'F' 'F' 'F' 'H' 'F' 'F' 'F' 'F';\n",
    "        'F' 'H' 'H' 'F' 'F' 'F' 'H' 'F';\n",
    "        'F' 'H' 'F' 'F' 'H' 'F' 'H' 'F';\n",
    "        'F' 'F' 'F' 'H' 'F' 'F' 'F' 'G';];\n",
    "\n",
    "function get_grid(dim, p_holes, seed = 234)\n",
    "    Random.seed!(seed)\n",
    "    grid = [rand() < p_holes ? 'H' : 'F' for i in 1:dim, j in 1:dim]\n",
    "    grid[1,1] = 'S'\n",
    "    grid[end,end] = 'G'\n",
    "    return grid\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "#Transition matrix T(s',a,s) - from state s to successor state s' by taking action a \n",
    "#assuming that ice is slippery and probability of moving forward is equal to 0.8, \n",
    "#probabilities of moving left or right are equal to 0.1\n",
    "\n",
    "function transition_matrix(grid, actions = actions)\n",
    "    T = zeros(length(grid),length(actions),length(grid))\n",
    "    i2s = CartesianIndices(grid)\n",
    "    s2i = LinearIndices(grid)\n",
    "    for i = 1:length(grid)\n",
    "        if !(grid[i] == 'H' || grid[i] == 'G')\n",
    "            index = i2s[i]\n",
    "            for j = 1:length(actions)\n",
    "                indices = Tuple(index) .+ actions[j]\n",
    "                if all(in.( indices, (1:size(grid,1), 1:size(grid,2)))) \n",
    "                    k = s2i[indices...]\n",
    "                    T[k,j,i] += 0.8\n",
    "                    if actions[j][1] == 0\n",
    "                        for l in [-1,1]\n",
    "                            ind = Tuple(index) .+ (l,0)\n",
    "                            if all(in.( ind, (1:size(grid,1), 1:size(grid,2)))) \n",
    "                                T[s2i[ind...],j,i] += 0.1\n",
    "                            else\n",
    "                                T[i,j,i] += 0.1\n",
    "                            end\n",
    "                        end\n",
    "                    else\n",
    "                        for l in [-1,1]\n",
    "                            ind = Tuple(index) .+ (0,l)\n",
    "                            if all(in.( ind, (1:size(grid,1), 1:size(grid,2))))\n",
    "                                T[s2i[ind...],j,i] += 0.1\n",
    "                            else\n",
    "                                T[i,j,i] += 0.1\n",
    "                            end\n",
    "                        end\n",
    "                    end\n",
    "                else\n",
    "                    T[i,j,i] += 0.8\n",
    "                    if actions[j][1] == 0\n",
    "                        for l in [-1,1]\n",
    "                            ind = Tuple(index) .+ (l,0)\n",
    "                            if all(in.( ind, (1:size(grid,1), 1:size(grid,2)))) \n",
    "                                T[s2i[ind...],j,i] += 0.1\n",
    "                            else\n",
    "                                T[i,j,i] += 0.1\n",
    "                            end\n",
    "                        end\n",
    "                    else\n",
    "                        for l in [-1,1]\n",
    "                            ind = Tuple(index) .+ (0,l)\n",
    "                            if all(in.( ind, (1:size(grid,1), 1:size(grid,2)))) \n",
    "                                T[s2i[ind...],j,i] += 0.1\n",
    "                            else\n",
    "                                T[i,j,i] += 0.1\n",
    "                            end\n",
    "                        end\n",
    "                    end\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return T\n",
    "end\n",
    "\n",
    "function reward_matrix(grid, rewards = rewards)\n",
    "    R = zeros(size(grid))\n",
    "    for i = 1:length(grid)\n",
    "        R[i] = rewards[grid[i]]\n",
    "    end\n",
    "    return R\n",
    "end\n",
    "\n",
    "\n",
    "random_policy(grid,actions = actions) = rand(1:length(actions),size(grid))\n",
    "\n",
    "\n",
    "function print_policy(P, grid, arrows = arrows)\n",
    "    Policy = rand(Char,size(grid))\n",
    "    for i = 1:length(grid)\n",
    "        if grid[i] == 'F' || grid[i] == 'S' \n",
    "            Policy[i] = arrows[P[i]]\n",
    "        elseif grid[i] == 'H' \n",
    "            Policy[i] = '⦷'\n",
    "        else\n",
    "            Policy[i] = grid[i]\n",
    "        end\n",
    "    end\n",
    "    return Policy\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557263f5",
   "metadata": {},
   "source": [
    "### Policy Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ba534bd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "evaluate! (generic function with 2 methods)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#synchronous DP\n",
    "function evaluate!(P, v, v₁, R, T, β)\n",
    "    for s = 1:length(v)\n",
    "        v₁[s]= R[s] + β * sum(v .*  T[:,P[s],s])\n",
    "    end\n",
    "end \n",
    "\n",
    "#in-place DP\n",
    "function evaluate!(P, v, R, T, β)\n",
    "    for s = 1:length(v)\n",
    "        v[s]= R[s] + β * sum(v .*  T[:,P[s],s])\n",
    "    end\n",
    "end "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cff4b0",
   "metadata": {},
   "source": [
    "### Policy Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c2e30d7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "evaluate_policy (generic function with 1 method)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function evaluate_policy(grid,P;\n",
    "                        β = 0.999, ϵ=0.0001, \n",
    "                        actions = actions)\n",
    "    iter = 0\n",
    "    T = transition_matrix(grid)\n",
    "    R = reward_matrix(grid)\n",
    "    v₁ = zeros(length(grid))\n",
    "    while true\n",
    "        iter += 1\n",
    "        v = deepcopy(v₁)\n",
    "        evaluate!(P, v, v₁, R, T, β)\n",
    "        #@info v₁\n",
    "        δ = maximum(abs.(v₁ - v)) \n",
    "        δ < ϵ * (1 - β) / β && break \n",
    "    end \n",
    "    \n",
    "    println(\"Iterations: $(iter)\")\n",
    "    return reshape(v₁,size(grid)),  print_policy(P, grid)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b12d431d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations: 2148\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([-5.14908660773023 -7.985478441301969 -8.806092629243011 -9.214356335655268; -4.738417925735517 -1.0 -1.913123221414557 -1.0; -4.494073679299217 -2.083805322378003 -1.8437895755390197 -1.0; -1.0 -1.7525685668096453 0.6277140572427211 1.0], ['⇓' '⇒' '⇑' '⇑'; '⇑' '⦷' '⇒' '⦷'; '⇑' '⇓' '⇐' '⦷'; '⦷' '⇑' '⇒' 'G'])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v, p = evaluate_policy(grid4x4, random_policy(grid4x4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c0b0e389",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×4 Matrix{Float64}:\n",
       " -5.14909  -7.98548  -8.80609   -9.21436\n",
       " -4.73842  -1.0      -1.91312   -1.0\n",
       " -4.49407  -2.08381  -1.84379   -1.0\n",
       " -1.0      -1.75257   0.627714   1.0"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "30ce5833",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×4 Matrix{Char}:\n",
       " '⇓'  '⇒'  '⇑'  '⇑'\n",
       " '⇑'  '⦷'  '⇒'  '⦷'\n",
       " '⇑'  '⇓'  '⇐'  '⦷'\n",
       " '⦷'  '⇑'  '⇒'  'G'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729351b6",
   "metadata": {},
   "source": [
    "### Value Iteration Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5232d9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations: 55\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#value iteration algorithm\n",
    "\n",
    "function get_policy(v, T, actions = actions)\n",
    "    P = rand(Int,length(v))\n",
    "    for s = 1:length(v)\n",
    "        actions_vector = zeros(length(actions))\n",
    "        for i = 1:length(actions)\n",
    "            actions_vector[i] = sum(v .* T[:,i,s])\n",
    "        end\n",
    "        P[s] = argmax(actions_vector)\n",
    "    end \n",
    "    return P\n",
    "end\n",
    "\n",
    "function update_values!(v₁, v, T,R,β, actions = actions)\n",
    "    for s = 1:length(v₁)\n",
    "        actions_vector = zeros(length(actions))\n",
    "        for a = 1:length(actions)\n",
    "            actions_vector[a] = sum(v .* T[:,a,s])\n",
    "        end\n",
    "        v₁[s] = R[s] +  β * maximum(actions_vector)\n",
    "    end\n",
    "end\n",
    "\n",
    " function value_iteration(grid,β = 0.999, ϵ=0.0001, actions = actions)\n",
    "    iter = 0\n",
    "    T = transition_matrix(grid)\n",
    "    R = reward_matrix(grid)\n",
    "    v₁ = zeros(length(grid))\n",
    "    while true\n",
    "        iter += 1\n",
    "        v = deepcopy(v₁)\n",
    "        update_values!(v₁,v, T,R,β)\n",
    "        δ = maximum( abs.(v₁ - v))  \n",
    "        δ < ϵ * (1 - β) / β && break\n",
    "    end\n",
    "    P = get_policy(v₁, T)\n",
    "    println(\"Iterations: $(iter)\")\n",
    "    return reshape(v₁,(size(grid))), print_policy(P, grid)\n",
    "end\n",
    "\n",
    "\n",
    "vᵥ, pᵥ =  value_iteration(grid8x8);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "70a31aa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8×8 Matrix{Float64}:\n",
       " -0.339438  -0.27149   -0.202868  …  -0.00621787   0.0505643  0.102791\n",
       " -0.385921  -0.322684  -0.25832       0.0399633    0.113759   0.172011\n",
       " -0.441663  -0.391518  -0.395462     -0.00332438   0.178408   0.24207\n",
       " -0.498583  -0.45843   -0.457666     -1.0          0.236042   0.312893\n",
       " -0.556933  -0.528808  -0.568494     -0.00192918   0.181735   0.385454\n",
       " -0.661038  -1.0       -1.0       …   0.0273081   -1.0        0.473963\n",
       " -0.753474  -1.0       -0.579584      0.238882    -1.0        0.721364\n",
       " -0.811142  -0.780109  -0.691036      0.611464     0.721364   1.0"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vᵥ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2259884d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8×8 Matrix{Char}:\n",
       " '⇒'  '⇒'  '⇒'  '⇒'  '⇒'  '⇒'  '⇓'  '⇓'\n",
       " '⇒'  '⇒'  '⇒'  '⇑'  '⇒'  '⇒'  '⇓'  '⇓'\n",
       " '⇑'  '⇑'  '⇑'  '⦷'  '⇒'  '⇒'  '⇒'  '⇓'\n",
       " '⇑'  '⇑'  '⇑'  '⇒'  '⇑'  '⦷'  '⇒'  '⇓'\n",
       " '⇑'  '⇑'  '⇑'  '⦷'  '⇒'  '⇒'  '⇒'  '⇓'\n",
       " '⇑'  '⦷'  '⦷'  '⇒'  '⇒'  '⇓'  '⦷'  '⇓'\n",
       " '⇑'  '⦷'  '⇒'  '⇑'  '⦷'  '⇓'  '⦷'  '⇓'\n",
       " '⇑'  '⇒'  '⇑'  '⦷'  '⇒'  '⇒'  '⇒'  'G'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pᵥ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111385fe",
   "metadata": {},
   "source": [
    "### Policy Iteration Algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "89b4b84a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations: 10\n"
     ]
    }
   ],
   "source": [
    "function evaluate!(π, v, v₁, R, T, β)\n",
    "    for s = 1:length(v)\n",
    "        v₁[s]= R[s] + β * sum(v .*  T[:,π[s],s])\n",
    "    end\n",
    "end \n",
    "\n",
    "function evaluate_policy!(v₁, T, R, π;\n",
    "                        β = 0.999, ϵ=0.0001,\n",
    "                        actions = actions)\n",
    "    while true        \n",
    "        v = deepcopy(v₁)\n",
    "        evaluate!(π, v, v₁, R, T, β)\n",
    "        δ = maximum(abs.(v₁ - v)) \n",
    "        δ < ϵ * (1 - β) / β && break \n",
    "    end \n",
    "end\n",
    "\n",
    "\n",
    "function improve_policy!(v, T, P, actions = actions)\n",
    "    done = true\n",
    "    for s = 1:length(v)\n",
    "        actions_vector = zeros(length(actions))\n",
    "        for a = 1:length(actions)\n",
    "            actions_vector[a] = sum(v .* T[:,a,s])\n",
    "        end\n",
    "        action = argmax(actions_vector)\n",
    "        action != P[s] && (P[s] = action; done = false)\n",
    "    end\n",
    "    return done\n",
    "end\n",
    "\n",
    "function policy_iteration(grid,β = 0.999, ϵ=0.0001)\n",
    "    iter = 1\n",
    "    T = transition_matrix(grid)\n",
    "    R = reward_matrix(grid)\n",
    "    v₁ = zeros(length(grid))\n",
    "    P = random_policy(grid)\n",
    "    done = false\n",
    "    while !done \n",
    "        iter += 1\n",
    "        done = true \n",
    "        evaluate_policy!(v₁, T, R, P,\n",
    "                        β = β, ϵ = ϵ,\n",
    "                        actions = actions)\n",
    "        done = improve_policy!(v₁, T, P)\n",
    "    end \n",
    "    println(\"Iterations: $(iter)\")\n",
    "    return reshape(v₁,size(grid)),  print_policy(P, grid)\n",
    "end\n",
    "\n",
    " vₚ, pₚ = policy_iteration(grid8x8);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "605caa86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3110764818247134e-7"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maximum(abs.(vₚ .- vᵥ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "fb1082df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "println(sum(pᵥ .== pₚ) / length(pᵥ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "160fef6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8×8 Matrix{Char}:\n",
       " '⇒'  '⇒'  '⇒'  '⇒'  '⇒'  '⇒'  '⇓'  '⇓'\n",
       " '⇒'  '⇒'  '⇒'  '⇑'  '⇒'  '⇒'  '⇓'  '⇓'\n",
       " '⇑'  '⇑'  '⇑'  '⦷'  '⇒'  '⇒'  '⇒'  '⇓'\n",
       " '⇑'  '⇑'  '⇑'  '⇒'  '⇑'  '⦷'  '⇒'  '⇓'\n",
       " '⇑'  '⇑'  '⇑'  '⦷'  '⇒'  '⇒'  '⇒'  '⇓'\n",
       " '⇑'  '⦷'  '⦷'  '⇒'  '⇒'  '⇓'  '⦷'  '⇓'\n",
       " '⇑'  '⦷'  '⇒'  '⇑'  '⦷'  '⇓'  '⦷'  '⇓'\n",
       " '⇑'  '⇒'  '⇑'  '⦷'  '⇒'  '⇒'  '⇒'  'G'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pₚ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cda120c",
   "metadata": {},
   "source": [
    "### Experiments - how rewards impact the optimal solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "41549780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations: 8\n",
      "Iterations: 35\n",
      "true\n",
      "Iterations: 11\n",
      "Iterations: 55\n",
      "true\n",
      "Iterations: 10\n",
      "Iterations: 481\n",
      "true\n",
      "Iterations: 17\n",
      "Iterations: 13106\n",
      "false\n",
      "Iterations: 21\n",
      "Iterations: 13805\n",
      "true\n"
     ]
    }
   ],
   "source": [
    "πs = []\n",
    "for α in [-0.2,-0.05,0.0,0.05,.1]\n",
    "    rewards = Dict('S' => α, 'G' => 1.0, 'H' => -1.0, 'F' => α);\n",
    "    vₚ, pₚ = policy_iteration(grid8x8);\n",
    "    vᵥ, pᵥ = value_iteration(grid8x8);\n",
    "    println(pₚ == pᵥ)\n",
    "    push!(πs,pₚ)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "075cc142",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8×8 Matrix{Char}:\n",
       " '⇒'  '⇒'  '⇓'  '⇓'  '⇓'  '⇓'  '⇓'  '⇓'\n",
       " '⇒'  '⇒'  '⇓'  '⇓'  '⇓'  '⇓'  '⇓'  '⇓'\n",
       " '⇒'  '⇒'  '⇒'  '⦷'  '⇐'  '⇒'  '⇓'  '⇓'\n",
       " '⇓'  '⇓'  '⇓'  '⇓'  '⇓'  '⦷'  '⇒'  '⇓'\n",
       " '⇓'  '⇓'  '⇓'  '⦷'  '⇒'  '⇒'  '⇒'  '⇓'\n",
       " '⇒'  '⦷'  '⦷'  '⇒'  '⇒'  '⇓'  '⦷'  '⇓'\n",
       " '⇒'  '⦷'  '⇑'  '⇒'  '⦷'  '⇓'  '⦷'  '⇓'\n",
       " '⇒'  '⇑'  '⇒'  '⦷'  '⇒'  '⇒'  '⇒'  'G'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "πs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4ced2113",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8×8 Matrix{Char}:\n",
       " '⇒'  '⇒'  '⇒'  '⇒'  '⇒'  '⇒'  '⇓'  '⇓'\n",
       " '⇒'  '⇒'  '⇒'  '⇑'  '⇒'  '⇒'  '⇓'  '⇓'\n",
       " '⇑'  '⇑'  '⇑'  '⦷'  '⇒'  '⇒'  '⇒'  '⇓'\n",
       " '⇑'  '⇑'  '⇑'  '⇒'  '⇑'  '⦷'  '⇒'  '⇓'\n",
       " '⇑'  '⇑'  '⇑'  '⦷'  '⇒'  '⇒'  '⇒'  '⇓'\n",
       " '⇑'  '⦷'  '⦷'  '⇒'  '⇒'  '⇓'  '⦷'  '⇓'\n",
       " '⇑'  '⦷'  '⇒'  '⇑'  '⦷'  '⇓'  '⦷'  '⇓'\n",
       " '⇑'  '⇒'  '⇑'  '⦷'  '⇒'  '⇒'  '⇒'  'G'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "πs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "845cf006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8×8 Matrix{Char}:\n",
       " '⇒'  '⇒'  '⇒'  '⇒'  '⇒'  '⇒'  '⇒'  '⇓'\n",
       " '⇒'  '⇒'  '⇑'  '⇑'  '⇑'  '⇒'  '⇒'  '⇓'\n",
       " '⇑'  '⇑'  '⇐'  '⦷'  '⇒'  '⇑'  '⇒'  '⇓'\n",
       " '⇑'  '⇑'  '⇐'  '⇐'  '⇑'  '⦷'  '⇒'  '⇓'\n",
       " '⇑'  '⇑'  '⇑'  '⦷'  '⇒'  '⇒'  '⇑'  '⇓'\n",
       " '⇐'  '⦷'  '⦷'  '⇒'  '⇑'  '⇐'  '⦷'  '⇒'\n",
       " '⇐'  '⦷'  '⇓'  '⇐'  '⦷'  '⇓'  '⦷'  '⇒'\n",
       " '⇑'  '⇓'  '⇐'  '⦷'  '⇒'  '⇒'  '⇓'  'G'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "πs[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "abeacab8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8×8 Matrix{Char}:\n",
       " '⇐'  '⇐'  '⇐'  '⇐'  '⇐'  '⇐'  '⇐'  '⇐'\n",
       " '⇑'  '⇑'  '⇑'  '⇑'  '⇑'  '⇑'  '⇑'  '⇑'\n",
       " '⇐'  '⇐'  '⇐'  '⦷'  '⇒'  '⇑'  '⇑'  '⇑'\n",
       " '⇐'  '⇐'  '⇐'  '⇐'  '⇑'  '⦷'  '⇒'  '⇑'\n",
       " '⇐'  '⇑'  '⇐'  '⦷'  '⇒'  '⇒'  '⇑'  '⇑'\n",
       " '⇐'  '⦷'  '⦷'  '⇒'  '⇑'  '⇐'  '⦷'  '⇒'\n",
       " '⇐'  '⦷'  '⇓'  '⇐'  '⦷'  '⇑'  '⦷'  '⇑'\n",
       " '⇐'  '⇓'  '⇐'  '⦷'  '⇒'  '⇑'  '⇐'  'G'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "πs[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4b4faeca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8×8 Matrix{Char}:\n",
       " '⇓'  '⇐'  '⇐'  '⇐'  '⇐'  '⇐'  '⇐'  '⇐'\n",
       " '⇑'  '⇑'  '⇑'  '⇑'  '⇑'  '⇑'  '⇑'  '⇑'\n",
       " '⇐'  '⇐'  '⇐'  '⦷'  '⇒'  '⇑'  '⇑'  '⇑'\n",
       " '⇐'  '⇐'  '⇐'  '⇐'  '⇑'  '⦷'  '⇒'  '⇑'\n",
       " '⇐'  '⇑'  '⇐'  '⦷'  '⇒'  '⇒'  '⇑'  '⇑'\n",
       " '⇐'  '⦷'  '⦷'  '⇒'  '⇑'  '⇐'  '⦷'  '⇒'\n",
       " '⇐'  '⦷'  '⇓'  '⇐'  '⦷'  '⇑'  '⦷'  '⇑'\n",
       " '⇐'  '⇓'  '⇐'  '⦷'  '⇒'  '⇑'  '⇐'  'G'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "πs[end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5106ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
