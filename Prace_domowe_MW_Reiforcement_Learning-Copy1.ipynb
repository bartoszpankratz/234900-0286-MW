{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prace Domowe - Modelowanie Wieloagentowe  Studia Sobotnio-Niedzielne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podane są polecenia, w ramach obowiązkowej pracy domowej należy wybrać i wykonać trzy z nich, pozostałe można traktować jako dodatkowe zadania. Każde zadanie punktowane jest za 30 punktów. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 1\n",
    "\n",
    "Korzystając ze środowiska <tt>Frozen Lake</tt> zaproponuj eksperyment, który pokaże złożoność obliczeniową prezentowanego na zajęciach rozwiązania problemu za pomocą programowania dynamicznego. Czy value iteration i policy iteration mają taka samą złożoność? Jakiego rzędu jest złożoność tych algorytmów? \n",
    "\n",
    "## Zadanie 2\n",
    "\n",
    "a) Zmodyfikuj prezentowane na zajęciach kody do algorytmu Monte Carlo On Policy w środowisku <tt>Frozen Lake</tt> (wersja z samodzielną implementacją środowiska, bez wykorzystania biblioteki `ReinforcementLearning`) tak aby:\n",
    "\n",
    "   - parametr eksploracji $\\epsilon$ malał liniowo z czasem.\n",
    "   - parametr eksploracji $\\epsilon$ malał wykładniczo z czasem.\n",
    "   - zamiast parametru eksploracji $\\epsilon$ agent wykorzystywał górną granicę ufności.\n",
    "    \n",
    "które z proponowanych rozwiązań jest najefektywniejsze? Do oceny wykorzystaj dwie miary: ilość epizodów zakończonych sukcesem (dotarciem do mety) i przeciętna skumulowana nagroda agenta. Porównanie przeprowadź w horyzoncie 1000000 iteracji.\n",
    "\n",
    "## Zadanie 3\n",
    "\n",
    "Dla najlepszego rozwiązania z zadania 2 (lub dowolnie wybranej metody spadku parametru eksploracji z poprzedniego zadania): \n",
    "- przyjmij, że agent nauczył się odpowiedniej strategii gdy jego success rate na przestrzeni 100 ostatnich iteracji przekracza wartośc $0.8\\times(1-\\epsilon)$. Po osiągnięciu takiego wyniku niech agent przestanie się uczyć i zacznie eksploatować swoją wyuczoną strategię. Zmodyfikuj kod aby uwzględnić to założenie. \n",
    "- przeprowadź eksperyment, który porówna ile epizodów jest potrzebnych do nauczenia się strategii w powyższym przypadku z ilością iteracji potrzebnych do rozwiązania zadania korzystając z algorytmu policy iteration. Przyjmij następujące parametry: wielkość jezior $dim \\in [100,200,300,\\dots,10\\_000]$ i $p\\_holes = 0.2$\n",
    "\n",
    "## Zadanie 4\n",
    "\n",
    "W zaprezentowanym na ćwiczeniach środowisku <tt>Frozen Lake</tt> lub <tt>Blackjack</tt> zaimplementuj algorytm Monte Carlo Off Policy Learning.\n",
    "\n",
    "\n",
    "## Zadanie 5\n",
    "\n",
    "Zmodyfikuj kod od przykładu <tt>Blackjack</tt> on policy tak żeby kod uaktualniał wartość funkcji $Q(s,a)$ za każdą wizytą (<i>every visit</i>) zamiast uaktualniania przy każdej wizycie (<i>first visit</i>) tak jak jest teraz. W jaki sposób zmienia to oszacowania funkcji wartości i strategię? Odpowiedz na pytanie rozpatrując różne horyzonty czasowe.\n",
    "\n",
    "\n",
    "\n",
    "## Zadanie 6\n",
    "\n",
    "Na zajęciach pokazaliśmy, że dla stałej wartości parametru eksploracji $\\epsilon$ algorytm $SARSA$ uczy się rozwiązania suboptymalnego. Zmodyfikuj parametr $\\epsilon$ tak aby malał z czasem. Czy w takim wypadku $SARSA$ uczy się rozwiązania optymalnego? Jak wyglądają przeciętne nagrody dla obu algorytmów ($SARSA$ i $Q - learning$) gdy $\\epsilon$ maleje? Porównaj je na wykresie z wynikami z zajęć.\n",
    "\n",
    "## Zadanie 7\n",
    "\n",
    "W środowisku <tt>Frozen Lake</tt> (wykorzystaj wersję z biblioteki `ReinforcementLearning`, ewentualnie zmodyfikuj nagrody tak aby przyjmowały nastepujące wartości: <tt>rewards = Dict('S' => 0.0, 'G' => 1.0, 'H' => 0.0, 'F' => 0.0)</tt>) zaimplementuj algorytm $SARSA(\\lambda)$ ze śladami wybieralności (<i>eligibility  traces</i>). Porównaj jego efektywność z  bazowym algorytmem $SARSA(0)$ dla różnych wartości parametru $\\lambda$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.0",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
