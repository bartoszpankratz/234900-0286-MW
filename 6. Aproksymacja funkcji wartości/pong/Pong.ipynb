{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21e95d74",
   "metadata": {},
   "source": [
    "# Deep Q Learning Pong\n",
    "\n",
    "Based on the [Julia implementation](https://juliareinforcementlearning.org/docs/experiments/experiments/DQN/Dopamine_DQN_Atari/#Dopamine\\\\_DQN\\\\_Atari(pong)) of the model from the seminal paper [Playing Atari with Deep Reinforcement Learning](https://arxiv.org/abs/1312.5602). \n",
    "To run it properly, it must be in the same working directory as <tt>Manifest.toml</tt> and <tt>Project.toml</tt> files.\n",
    "\n",
    "Other interesting models are avalaible [here](https://juliareinforcementlearning.org/docs/rlzoo/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a78d46",
   "metadata": {},
   "source": [
    "Packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "155e1125",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Random\n",
    "#RL packages:\n",
    "using ReinforcementLearning\n",
    "using ArcadeLearningEnvironment\n",
    "#Neural Network:\n",
    "using Flux\n",
    "using Flux: params\n",
    "using Flux.Losses: huber_loss\n",
    "#for logging:\n",
    "using Setfield\n",
    "using Statistics\n",
    "using Dates\n",
    "using Logging\n",
    "using TensorBoardLogger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8b33be",
   "metadata": {},
   "source": [
    "Name of a selected game:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "857ad141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"pong\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = \"pong\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd36aa61",
   "metadata": {},
   "source": [
    "Random numbers and logging directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f655f44a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TBLogger:\n",
       "\t- Log level     : Info\n",
       "\t- Current step  : 0\n",
       "\t- Output        : /root/Pong/checkpoints/dopamine_DQN_atari_pong_2023_05_10_00_47_25/tb_log\n",
       "\t- open files    : 1\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rng = Random.GLOBAL_RNG\n",
    "seed = 1234\n",
    "Random.seed!(rng, seed)\n",
    "t = Dates.format(now(), \"yyyy_mm_dd_HH_MM_SS\")\n",
    "save_dir = joinpath(pwd(), \"checkpoints\", \"dopamine_DQN_atari_$(name)_$(t)\")\n",
    "lg = TBLogger(joinpath(save_dir, \"tb_log\"), min_level = Logging.Info)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7937d2d",
   "metadata": {},
   "source": [
    "Auxilliary functions - image transformations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37013abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "using ImageTransformations: imresize!\n",
    "\n",
    "\"\"\"\n",
    "    ResizeImage(img::Array{T, N})\n",
    "    ResizeImage(dims::Int...) -> ResizeImage(Float32, dims...)\n",
    "    ResizeImage(T::Type{<:Number}, dims::Int...)\n",
    "\n",
    "By default the `BSpline(Linear())`` method is used to resize the `state` field\n",
    "of an observation to size of `img` (or `dims`). In some other packages, people\n",
    "use the\n",
    "[`cv2.INTER_AREA`](https://github.com/google/dopamine/blob/2a7d91d2831ca28cea0d3b0f4d5c7a7107e846ab/dopamine/discrete_domains/atari_lib.py#L511-L513),\n",
    "which is not supported in `ImageTransformations.jl` yet.\n",
    "\"\"\"\n",
    "\n",
    "struct ResizeImage{T,N}\n",
    "    img::Array{T,N}\n",
    "end\n",
    "\n",
    "ResizeImage(dims::Int...) = ResizeImage(Float32, dims...)\n",
    "ResizeImage(T::Type{<:Number}, dims::Int...) = ResizeImage(Array{T}(undef, dims))\n",
    "\n",
    "function (p::ResizeImage)(state::AbstractArray)\n",
    "    imresize!(p.img, state)\n",
    "    p.img\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00347276",
   "metadata": {},
   "source": [
    "Enivironment creating function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e57ff87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "atari_env_factory (generic function with 2 methods)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function atari_env_factory(\n",
    "    name,\n",
    "    state_size,\n",
    "    n_frames,\n",
    "    max_episode_steps = 100_000;\n",
    "    seed = nothing,\n",
    "    repeat_action_probability = 0.25,\n",
    "    n_replica = nothing, #USED FOR THE PARALLEL ENVIRONMENTS\n",
    ")\n",
    "    init(seed) =\n",
    "        RewardTransformedEnv(\n",
    "            StateCachedEnv(\n",
    "                StateTransformedEnv(\n",
    "                    AtariEnv(;\n",
    "                        name = string(name),\n",
    "                        grayscale_obs = true,\n",
    "                        noop_max = 30, #the maximum number of \"do nothing\" actions to be performed by the agent at the start of an episode.\n",
    "                        frame_skip = 4,\n",
    "                        terminal_on_life_loss = false,\n",
    "                        repeat_action_probability = repeat_action_probability,\n",
    "                        max_num_frames_per_episode = n_frames * max_episode_steps,\n",
    "                        color_averaging = false,\n",
    "                        full_action_space = false,\n",
    "                        seed = seed,\n",
    "                    );\n",
    "                    state_mapping=Chain(\n",
    "                        ResizeImage(state_size...),\n",
    "                        StackFrames(state_size..., n_frames)\n",
    "                    ),\n",
    "                    state_space_mapping= _ -> Space(fill(0..256, state_size..., n_frames))\n",
    "                )\n",
    "            );\n",
    "            reward_mapping = r -> clamp(r, -1, 1)\n",
    "        )\n",
    "\n",
    "    if isnothing(n_replica)\n",
    "        init(seed)\n",
    "    else\n",
    "        envs = [\n",
    "            init(isnothing(seed) ? nothing : hash(seed + i))\n",
    "            for i in 1:n_replica\n",
    "        ]\n",
    "        states = Flux.batch(state.(envs))\n",
    "        rewards = reward.(envs)\n",
    "        terminals = is_terminated.(envs)\n",
    "        A = Space([action_space(x) for x in envs])\n",
    "        S = Space(fill(0..255, size(states)))\n",
    "        MultiThreadEnv(envs, states, rewards, terminals, A, S, nothing)\n",
    "    end\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d6f750",
   "metadata": {},
   "source": [
    "Let us define learning constants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cf3d8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.6.0)\n",
      "[Powered by Stella]\n",
      "Use -help for help screen.\n",
      "Warning: couldn't load settings file: ./ale.cfg\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "#1 (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_FRAMES = 4\n",
    "STATE_SIZE = (48, 48)\n",
    "env = atari_env_factory(\n",
    "        name,\n",
    "        STATE_SIZE,\n",
    "        N_FRAMES;\n",
    "        seed = isnothing(seed) ? nothing : hash(seed + 1)\n",
    "    )\n",
    "N_ACTIONS = length(action_space(env))\n",
    "init = glorot_uniform(rng)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ace9a3",
   "metadata": {},
   "source": [
    "and create a neural network model (a \"brain\" of our agent):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c70ebbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "create_model (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_model() =\n",
    "        Chain(\n",
    "            x -> x ./ 255,\n",
    "            CrossCor((8, 8), N_FRAMES => 32, relu; stride = 4, pad = 2, init = init),\n",
    "            CrossCor((4, 4), 32 => 64, relu; stride = 2, pad = 2, init = init),\n",
    "            CrossCor((3, 3), 64 => 64, relu; stride = 1, pad = 1, init = init),\n",
    "            x -> reshape(x, :, size(x)[end]),\n",
    "            Dense(7 * 7 * 64, 512, relu; init = init),\n",
    "            Dense(512, N_ACTIONS; init = init),\n",
    "        ) |> gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fe4896",
   "metadata": {},
   "source": [
    "Now, we could create an agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a79c3fbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "typename(Agent)\n",
       "├─ policy => typename(QBasedPolicy)\n",
       "│  ├─ learner => typename(DQNLearner)\n",
       "│  │  ├─ approximator => typename(NeuralNetworkApproximator)\n",
       "│  │  │  ├─ model => typename(Chain)\n",
       "│  │  │  │  └─ layers\n",
       "│  │  │  │     ├─ 1\n",
       "│  │  │  │     │  └─ typename(var\"#11#13\")\n",
       "│  │  │  │     ├─ 2\n",
       "│  │  │  │     │  └─ typename(CrossCor)\n",
       "│  │  │  │     │     ├─ σ => typename(typeof(relu))\n",
       "│  │  │  │     │     ├─ weight => 8×8×4×32 CUDA.CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}\n",
       "│  │  │  │     │     ├─ bias => 32-element CUDA.CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}\n",
       "│  │  │  │     │     ├─ stride\n",
       "│  │  │  │     │     │  ├─ 1\n",
       "│  │  │  │     │     │  │  └─ 4\n",
       "│  │  │  │     │     │  └─ 2\n",
       "│  │  │  │     │     │     └─ 4\n",
       "│  │  │  │     │     ├─ pad\n",
       "│  │  │  │     │     │  ├─ 1\n",
       "│  │  │  │     │     │  │  └─ 2\n",
       "│  │  │  │     │     │  ├─ 2\n",
       "│  │  │  │     │     │  │  └─ 2\n",
       "│  │  │  │     │     │  ├─ 3\n",
       "│  │  │  │     │     │  │  └─ 2\n",
       "│  │  │  │     │     │  └─ 4\n",
       "│  │  │  │     │     │     └─ 2\n",
       "│  │  │  │     │     └─ dilation\n",
       "│  │  │  │     │        ├─ 1\n",
       "│  │  │  │     │        │  └─ 1\n",
       "│  │  │  │     │        └─ 2\n",
       "│  │  │  │     │           └─ 1\n",
       "│  │  │  │     ├─ 3\n",
       "│  │  │  │     │  └─ typename(CrossCor)\n",
       "│  │  │  │     │     ├─ σ => typename(typeof(relu))\n",
       "│  │  │  │     │     ├─ weight => 4×4×32×64 CUDA.CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}\n",
       "│  │  │  │     │     ├─ bias => 64-element CUDA.CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}\n",
       "│  │  │  │     │     ├─ stride\n",
       "│  │  │  │     │     │  ├─ 1\n",
       "│  │  │  │     │     │  │  └─ 2\n",
       "│  │  │  │     │     │  └─ 2\n",
       "│  │  │  │     │     │     └─ 2\n",
       "│  │  │  │     │     ├─ pad\n",
       "│  │  │  │     │     │  ├─ 1\n",
       "│  │  │  │     │     │  │  └─ 2\n",
       "│  │  │  │     │     │  ├─ 2\n",
       "│  │  │  │     │     │  │  └─ 2\n",
       "│  │  │  │     │     │  ├─ 3\n",
       "│  │  │  │     │     │  │  └─ 2\n",
       "│  │  │  │     │     │  └─ 4\n",
       "│  │  │  │     │     │     └─ 2\n",
       "│  │  │  │     │     └─ dilation\n",
       "│  │  │  │     │        ├─ 1\n",
       "│  │  │  │     │        │  └─ 1\n",
       "│  │  │  │     │        └─ 2\n",
       "│  │  │  │     │           └─ 1\n",
       "│  │  │  │     ├─ 4\n",
       "│  │  │  │     │  └─ typename(CrossCor)\n",
       "│  │  │  │     │     ├─ σ => typename(typeof(relu))\n",
       "│  │  │  │     │     ├─ weight => 3×3×64×64 CUDA.CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}\n",
       "│  │  │  │     │     ├─ bias => 64-element CUDA.CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}\n",
       "│  │  │  │     │     ├─ stride\n",
       "│  │  │  │     │     │  ├─ 1\n",
       "│  │  │  │     │     │  │  └─ 1\n",
       "│  │  │  │     │     │  └─ 2\n",
       "│  │  │  │     │     │     └─ 1\n",
       "│  │  │  │     │     ├─ pad\n",
       "│  │  │  │     │     │  ├─ 1\n",
       "│  │  │  │     │     │  │  └─ 1\n",
       "│  │  │  │     │     │  ├─ 2\n",
       "│  │  │  │     │     │  │  └─ 1\n",
       "│  │  │  │     │     │  ├─ 3\n",
       "│  │  │  │     │     │  │  └─ 1\n",
       "│  │  │  │     │     │  └─ 4\n",
       "│  │  │  │     │     │     └─ 1\n",
       "│  │  │  │     │     └─ dilation\n",
       "│  │  │  │     │        ├─ 1\n",
       "│  │  │  │     │        │  └─ 1\n",
       "│  │  │  │     │        └─ 2\n",
       "│  │  │  │     │           └─ 1\n",
       "│  │  │  │     ├─ 5\n",
       "│  │  │  │     │  └─ typename(var\"#12#14\")\n",
       "│  │  │  │     ├─ 6\n",
       "│  │  │  │     │  └─ typename(Dense)\n",
       "│  │  │  │     │     ├─ weight => 512×3136 CUDA.CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}\n",
       "│  │  │  │     │     ├─ bias => 512-element CUDA.CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}\n",
       "│  │  │  │     │     └─ σ => typename(typeof(relu))\n",
       "│  │  │  │     └─ 7\n",
       "│  │  │  │        └─ typename(Dense)\n",
       "│  │  │  │           ├─ weight => 6×512 CUDA.CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}\n",
       "│  │  │  │           ├─ bias => 6-element CUDA.CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}\n",
       "│  │  │  │           └─ σ => typename(typeof(identity))\n",
       "│  │  │  └─ optimizer => typename(ADAM)\n",
       "│  │  │     ├─ eta => 0.0001\n",
       "│  │  │     ├─ beta\n",
       "│  │  │     │  ├─ 1\n",
       "│  │  │     │  │  └─ 0.9\n",
       "│  │  │     │  └─ 2\n",
       "│  │  │     │     └─ 0.999\n",
       "│  │  │     ├─ epsilon => 1.0e-8\n",
       "│  │  │     └─ state => typename(IdDict)\n",
       "│  │  ├─ target_approximator => typename(NeuralNetworkApproximator)\n",
       "│  │  │  ├─ model => typename(Chain)\n",
       "│  │  │  │  └─ layers\n",
       "│  │  │  │     ├─ 1\n",
       "│  │  │  │     │  └─ typename(var\"#11#13\")\n",
       "│  │  │  │     ├─ 2\n",
       "│  │  │  │     │  └─ typename(CrossCor)\n",
       "│  │  │  │     │     ├─ σ => typename(typeof(relu))\n",
       "│  │  │  │     │     ├─ weight => 8×8×4×32 CUDA.CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}\n",
       "│  │  │  │     │     ├─ bias => 32-element CUDA.CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}\n",
       "│  │  │  │     │     ├─ stride\n",
       "│  │  │  │     │     │  ├─ 1\n",
       "│  │  │  │     │     │  │  └─ 4\n",
       "│  │  │  │     │     │  └─ 2\n",
       "│  │  │  │     │     │     └─ 4\n",
       "│  │  │  │     │     ├─ pad\n",
       "│  │  │  │     │     │  ├─ 1\n",
       "│  │  │  │     │     │  │  └─ 2\n",
       "│  │  │  │     │     │  ├─ 2\n",
       "│  │  │  │     │     │  │  └─ 2\n",
       "│  │  │  │     │     │  ├─ 3\n",
       "│  │  │  │     │     │  │  └─ 2\n",
       "│  │  │  │     │     │  └─ 4\n",
       "│  │  │  │     │     │     └─ 2\n",
       "│  │  │  │     │     └─ dilation\n",
       "│  │  │  │     │        ├─ 1\n",
       "│  │  │  │     │        │  └─ 1\n",
       "│  │  │  │     │        └─ 2\n",
       "│  │  │  │     │           └─ 1\n",
       "│  │  │  │     ├─ 3\n",
       "│  │  │  │     │  └─ typename(CrossCor)\n",
       "│  │  │  │     │     ├─ σ => typename(typeof(relu))\n",
       "│  │  │  │     │     ├─ weight => 4×4×32×64 CUDA.CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}\n",
       "│  │  │  │     │     ├─ bias => 64-element CUDA.CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}\n",
       "│  │  │  │     │     ├─ stride\n",
       "│  │  │  │     │     │  ├─ 1\n",
       "│  │  │  │     │     │  │  └─ 2\n",
       "│  │  │  │     │     │  └─ 2\n",
       "│  │  │  │     │     │     └─ 2\n",
       "│  │  │  │     │     ├─ pad\n",
       "│  │  │  │     │     │  ├─ 1\n",
       "│  │  │  │     │     │  │  └─ 2\n",
       "│  │  │  │     │     │  ├─ 2\n",
       "│  │  │  │     │     │  │  └─ 2\n",
       "│  │  │  │     │     │  ├─ 3\n",
       "│  │  │  │     │     │  │  └─ 2\n",
       "│  │  │  │     │     │  └─ 4\n",
       "│  │  │  │     │     │     └─ 2\n",
       "│  │  │  │     │     └─ dilation\n",
       "│  │  │  │     │        ├─ 1\n",
       "│  │  │  │     │        │  └─ 1\n",
       "│  │  │  │     │        └─ 2\n",
       "│  │  │  │     │           └─ 1\n",
       "│  │  │  │     ├─ 4\n",
       "│  │  │  │     │  └─ typename(CrossCor)\n",
       "│  │  │  │     │     ├─ σ => typename(typeof(relu))\n",
       "│  │  │  │     │     ├─ weight => 3×3×64×64 CUDA.CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}\n",
       "│  │  │  │     │     ├─ bias => 64-element CUDA.CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}\n",
       "│  │  │  │     │     ├─ stride\n",
       "│  │  │  │     │     │  ├─ 1\n",
       "│  │  │  │     │     │  │  └─ 1\n",
       "│  │  │  │     │     │  └─ 2\n",
       "│  │  │  │     │     │     └─ 1\n",
       "│  │  │  │     │     ├─ pad\n",
       "│  │  │  │     │     │  ├─ 1\n",
       "│  │  │  │     │     │  │  └─ 1\n",
       "│  │  │  │     │     │  ├─ 2\n",
       "│  │  │  │     │     │  │  └─ 1\n",
       "│  │  │  │     │     │  ├─ 3\n",
       "│  │  │  │     │     │  │  └─ 1\n",
       "│  │  │  │     │     │  └─ 4\n",
       "│  │  │  │     │     │     └─ 1\n",
       "│  │  │  │     │     └─ dilation\n",
       "│  │  │  │     │        ├─ 1\n",
       "│  │  │  │     │        │  └─ 1\n",
       "│  │  │  │     │        └─ 2\n",
       "│  │  │  │     │           └─ 1\n",
       "│  │  │  │     ├─ 5\n",
       "│  │  │  │     │  └─ typename(var\"#12#14\")\n",
       "│  │  │  │     ├─ 6\n",
       "│  │  │  │     │  └─ typename(Dense)\n",
       "│  │  │  │     │     ├─ weight => 512×3136 CUDA.CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}\n",
       "│  │  │  │     │     ├─ bias => 512-element CUDA.CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}\n",
       "│  │  │  │     │     └─ σ => typename(typeof(relu))\n",
       "│  │  │  │     └─ 7\n",
       "│  │  │  │        └─ typename(Dense)\n",
       "│  │  │  │           ├─ weight => 6×512 CUDA.CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}\n",
       "│  │  │  │           ├─ bias => 6-element CUDA.CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}\n",
       "│  │  │  │           └─ σ => typename(typeof(identity))\n",
       "│  │  │  └─ optimizer => typename(Nothing)\n",
       "│  │  ├─ loss_func => typename(typeof(huber_loss))\n",
       "│  │  ├─ min_replay_history => 5000\n",
       "│  │  ├─ update_freq => 4\n",
       "│  │  ├─ update_step => 0\n",
       "│  │  ├─ target_update_freq => 4000\n",
       "│  │  ├─ sampler => typename(NStepBatchSampler)\n",
       "│  │  │  ├─ γ => 0.99\n",
       "│  │  │  ├─ n => 1\n",
       "│  │  │  ├─ batch_size => 32\n",
       "│  │  │  ├─ stack_size => 4\n",
       "│  │  │  ├─ rng => typename(Random._GLOBAL_RNG)\n",
       "│  │  │  └─ cache => typename(Nothing)\n",
       "│  │  ├─ rng => typename(Random._GLOBAL_RNG)\n",
       "│  │  ├─ loss => 0.0\n",
       "│  │  └─ is_enable_double_DQN => true\n",
       "│  └─ explorer => typename(EpsilonGreedyExplorer)\n",
       "│     ├─ ϵ_stable => 0.01\n",
       "│     ├─ ϵ_init => 1.0\n",
       "│     ├─ warmup_steps => 0\n",
       "│     ├─ decay_steps => 250000\n",
       "│     ├─ step => 1\n",
       "│     ├─ rng => typename(Random._GLOBAL_RNG)\n",
       "│     └─ is_training => true\n",
       "└─ trajectory => typename(Trajectory)\n",
       "   └─ traces => typename(NamedTuple)\n",
       "      ├─ state => 48×48×0 CircularArrayBuffers.CircularArrayBuffer{Float32, 3, Array{Float32, 3}}\n",
       "      ├─ action => 0-element CircularArrayBuffers.CircularVectorBuffer{Int64, Vector{Int64}}\n",
       "      ├─ reward => 0-element CircularArrayBuffers.CircularVectorBuffer{Float32, Vector{Float32}}\n",
       "      └─ terminal => 0-element CircularArrayBuffers.CircularVectorBuffer{Bool, Vector{Bool}}\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = Agent(\n",
    "        policy = QBasedPolicy(\n",
    "            learner = DQNLearner(\n",
    "                approximator = NeuralNetworkApproximator(\n",
    "                    model = create_model(),\n",
    "                    optimizer = ADAM(0.0001),),  \n",
    "                target_approximator = NeuralNetworkApproximator(model = create_model()),\n",
    "                update_freq = 4,\n",
    "                γ = 0.99f0,\n",
    "                update_horizon = 1,\n",
    "                batch_size = 32,\n",
    "                stack_size = N_FRAMES,\n",
    "                min_replay_history = 5_000,\n",
    "                loss_func = huber_loss,\n",
    "                target_update_freq = 4_000,\n",
    "                rng = rng,\n",
    "            ),\n",
    "            explorer = EpsilonGreedyExplorer(\n",
    "                ϵ_init = 1.0,\n",
    "                ϵ_stable = 0.01,\n",
    "                decay_steps = 250_000,\n",
    "                kind = :linear,\n",
    "                rng = rng,\n",
    "            ),\n",
    "        ),\n",
    "        trajectory = CircularArraySARTTrajectory(\n",
    "            capacity =  50_000,\n",
    "            state = Matrix{Float32} => STATE_SIZE,\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3ff729",
   "metadata": {},
   "source": [
    "Before running we will also define custom hooks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "491fd099",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Total reward per episode before reward reshaping\"\n",
    "Base.@kwdef mutable struct TotalOriginalRewardPerEpisode <: AbstractHook\n",
    "    rewards::Vector{Float64} = Float64[]\n",
    "    reward::Float64 = 0.0\n",
    "end\n",
    "\n",
    "function (hook::TotalOriginalRewardPerEpisode)(\n",
    "    ::PostActStage,\n",
    "    agent,\n",
    "    env::RewardTransformedEnv,\n",
    ")\n",
    "    hook.reward += reward(env.env)\n",
    "end\n",
    "\n",
    "function (hook::TotalOriginalRewardPerEpisode)(::PostEpisodeStage, agent, env)\n",
    "    push!(hook.rewards, hook.reward)\n",
    "    hook.reward = 0\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f6fec1",
   "metadata": {},
   "source": [
    "and for parallel version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00cd0538",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Total reward of each inner env per episode before reward reshaping\"\n",
    "struct TotalBatchOriginalRewardPerEpisode <: AbstractHook\n",
    "    rewards::Vector{Vector{Float64}}\n",
    "    reward::Vector{Float64}\n",
    "end\n",
    "\n",
    "function TotalBatchOriginalRewardPerEpisode(batch_size::Int)\n",
    "    TotalBatchOriginalRewardPerEpisode([Float64[] for _ in 1:batch_size], zeros(batch_size))\n",
    "end\n",
    "\n",
    "function (hook::TotalBatchOriginalRewardPerEpisode)(\n",
    "    ::PostActStage,\n",
    "    agent,\n",
    "    env::MultiThreadEnv{<:RewardTransformedEnv},\n",
    ")\n",
    "    for (i, e) in enumerate(env.envs)\n",
    "        hook.reward[i] += reward(e.env)\n",
    "        if is_terminated(e)\n",
    "            push!(hook.rewards[i], hook.reward[i])\n",
    "            hook.reward[i] = 0.0\n",
    "        end\n",
    "    end\n",
    "end "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f08eb6f",
   "metadata": {},
   "source": [
    "Finally our hook will look like that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4e477f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TotalOriginalRewardPerEpisode(Float64[], 0.0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EVALUATION_FREQ = 200_000\n",
    "MAX_EPISODE_STEPS_EVAL = 100_000\n",
    "step_per_episode = StepsPerEpisode()\n",
    "reward_per_episode = TotalOriginalRewardPerEpisode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c165b47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ComposedHook{Tuple{StepsPerEpisode, TotalOriginalRewardPerEpisode, DoEveryNStep{var\"#18#24\"}, DoEveryNEpisode{PostEpisodeStage, var\"#20#26\"}, DoEveryNStep{var\"#22#28\"}}}((StepsPerEpisode(Int64[], 0), TotalOriginalRewardPerEpisode(Float64[], 0.0), DoEveryNStep{var\"#18#24\"}(var\"#18#24\"(), 1, 0), DoEveryNEpisode{PostEpisodeStage, var\"#20#26\"}(var\"#20#26\"(), 1, 0), DoEveryNStep{var\"#22#28\"}(var\"#22#28\"(), 200000, 0)))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hook = ComposedHook(\n",
    "    step_per_episode,\n",
    "    reward_per_episode,\n",
    "    DoEveryNStep() do t, agent, env\n",
    "        with_logger(lg) do #save the value of the loss function\n",
    "            @info \"training\" loss = agent.policy.learner.loss \n",
    "        end\n",
    "    end,\n",
    "    DoEveryNEpisode() do t, agent, env\n",
    "        with_logger(lg) do #basic statistics of training mode: step_per_episode and reward_per_episode\n",
    "            @info \"training\" episode_length = step_per_episode.steps[end] reward = reward_per_episode.rewards[end] log_step_increment = 0\n",
    "        end\n",
    "    end,\n",
    "    DoEveryNStep(;n=EVALUATION_FREQ) do t, agent, env #evaluation mode\n",
    "        @info \"evaluating agent at $t step...\"\n",
    "        p = agent.policy\n",
    "        p = @set p.explorer = EpsilonGreedyExplorer(0.001; rng = rng)  # set evaluation epsilon\n",
    "        h = ComposedHook(\n",
    "            TotalOriginalRewardPerEpisode(),\n",
    "            StepsPerEpisode(),\n",
    "            )\n",
    "        s = @elapsed run(\n",
    "            p,\n",
    "            atari_env_factory(name,\n",
    "                            STATE_SIZE,\n",
    "                            N_FRAMES,\n",
    "                            MAX_EPISODE_STEPS_EVAL;\n",
    "                            seed = isnothing(seed) ? nothing : hash(seed + t)),\n",
    "                            StopAfterStep(10_000; is_show_progress = false),\n",
    "                            h)\n",
    "            avg_score = mean(h[1].rewards[1:end-1])\n",
    "            avg_length = mean(h[2].steps[1:end-1])\n",
    "        @info \"finished evaluating agent in $s seconds\" avg_length = avg_length avg_score = avg_score\n",
    "        with_logger(lg) do\n",
    "            @info \"evaluating\" avg_length = avg_length avg_score = avg_score log_step_increment = 0\n",
    "        end\n",
    "    end,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7809f3bf",
   "metadata": {},
   "source": [
    "it will save the main learning metrics in the [TensorBoardLogger](https://www.tensorflow.org/tensorboard/get_started) format. Lastly, stop condition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d27e8ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StopAfterStep{ProgressMeter.Progress}(8000000, 1, ProgressMeter.Progress(8000000, ReentrantLock(nothing, 0x00000000, 0x00, Base.GenericCondition{Base.Threads.SpinLock}(Base.InvasiveLinkedList{Task}(nothing, nothing), Base.Threads.SpinLock(0)), (8, 139718739227392, 8589934593)), 1.0, 1, 1.683694079824522e9, 1.683694079824522e9, 1.683694079824522e9, false, \"Progress: \", nothing, ProgressMeter.BarGlyphs('|', '█', ['▏', '▎', '▍', '▌', '▋', '▊', '▉'], ' ', '|'), :green, IJulia.IJuliaStdio{Base.PipeEndpoint}(IOContext(Base.PipeEndpoint(RawFD(44) open, 0 bytes waiting))), 0, 0, 0, true, false, 1, 1, Int64[]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_condition = StopAfterStep(8_000_000,is_show_progress=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c53fe7",
   "metadata": {},
   "source": [
    "Learning time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692dc4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "run(agent, env, stop_condition, hook)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd722cc",
   "metadata": {},
   "source": [
    "Now, we will save the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60580b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux: params\n",
    "using BSON\n",
    "\n",
    "ps = params(agent.policy) |> cpu\n",
    "\n",
    "BSON.@save \"pong_weights.bson\" ps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e74c1f",
   "metadata": {},
   "source": [
    "### Results Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ea9840",
   "metadata": {},
   "source": [
    "Firstly, we will create a simple, animated gif to take a look at the agent gaming skills."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0fb68eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots, Images, ImageCore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f16e4079",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Frames of pong game\"\n",
    "Base.@kwdef mutable struct AnimateGame <: AbstractHook\n",
    "    plots = []\n",
    "end\n",
    "\n",
    "function (hook::AnimateGame)(\n",
    "    ::PostActStage, agent,\n",
    "    env::RewardTransformedEnv,\n",
    ")\n",
    "    p =  plot(transpose(Gray.(state(env.env)[:,:,1]./255)),ticks = false, showaxis = false)\n",
    "    push!(hook.plots, p)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe369d97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "anim_game (generic function with 1 method)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function anim_game(policy, gif_name; seed = 1234, MAX_EPISODES = 1, MAX_EPISODE_STEPS_ANIM = 3000, \n",
    "        FPS = 23, gif_length = 5)\n",
    "    policy = @set policy.explorer = EpsilonGreedyExplorer(0.001; rng = rng)  # set evaluation epsilon\n",
    "    hook = AnimateGame()\n",
    "    s = run(policy,\n",
    "        atari_env_factory(name,\n",
    "        STATE_SIZE,\n",
    "        N_FRAMES,\n",
    "        MAX_EPISODE_STEPS_ANIM;\n",
    "        seed = seed),\n",
    "        StopAfterStep(MAX_EPISODES * MAX_EPISODE_STEPS_ANIM, is_show_progress = false),\n",
    "        hook,)\n",
    "    first_frame = rand(1:length(hook.plots) - FPS *gif_length);\n",
    "    gif_frames = hook.plots[first_frame:(first_frame + FPS *gif_length)];\n",
    "    anim = @animate for frame in gif_frames\n",
    "        plot(frame)\n",
    "    end\n",
    "    anim\n",
    "    gif(anim, gif_name, fps = FPS)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1040858f",
   "metadata": {},
   "source": [
    "Now, test for random policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0248db1e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_policy = QBasedPolicy(\n",
    "            learner = DQNLearner(\n",
    "                approximator = NeuralNetworkApproximator(\n",
    "                    model = create_model(),\n",
    "                    optimizer = ADAM(0.0001),),  \n",
    "                target_approximator = NeuralNetworkApproximator(model = create_model()),\n",
    "                update_freq = 4,\n",
    "                γ = 0.99f0,\n",
    "                update_horizon = 1,\n",
    "                batch_size = 32,\n",
    "                stack_size = N_FRAMES,\n",
    "                min_replay_history = 5_000,\n",
    "                loss_func = huber_loss,\n",
    "                target_update_freq = 4_000,\n",
    "                rng = rng,\n",
    "            ),\n",
    "            explorer = EpsilonGreedyExplorer(\n",
    "                ϵ_init = 1.0,\n",
    "                ϵ_stable = 0.01,\n",
    "                decay_steps = 250_000,\n",
    "                kind = :linear,\n",
    "                rng = rng,\n",
    "            ),\n",
    "        );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf4f37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "anim_game(test_policy, \"pong_before.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef138a4",
   "metadata": {},
   "source": [
    "and for the trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4039d78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "using BSON, Zygote, CUDA, Flux\n",
    "\n",
    "ps = params(agent.policy)\n",
    "\n",
    "BSON.@load \"ps.bson\" ps \n",
    "ps = gpu(ps)\n",
    "#Flux.loadparams!(agent.policy,ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c90398",
   "metadata": {},
   "outputs": [],
   "source": [
    "anim_game(agent.policy, \"pong_after.gif\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.5",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
